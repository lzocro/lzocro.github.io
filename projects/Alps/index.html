<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Algorithms with Predictions | Lorenzo Croissant</title> <meta name="author" content="Lorenzo Croissant"/> <meta name="description" content="How to use your excel spreadsheet to make decisions."/> <meta name="keywords" content="lorenzo-croissant, lorenzo, croissant, control-theory, RL, reinforcement-learning, auction, auctions, auction-theory, jekyll, jekyll-theme, academic-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü•ê</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://lzocro.github.io/projects/Alps/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],tags:"ams",maxBuffer:10240,maxMacros:10240}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <d-front-matter> <script async type="text/json">{
      "title": "Algorithms with Predictions",
      "description": "How to use your excel spreadsheet to make decisions.",
      "published": "July 1, 2025",
      "authors": [
        {
          "author": "Lorenzo Croissant",
          "authorURL": "",
          "affiliations": [
            {
              "name": "CREST, ENSAE, & Inria FairPlay",
              "url": ""
            }
          ]
        },
        {
          "author": "Ziyad Benomar",
          "authorURL": "",
          "affiliations": [
            {
              "name": "CREST, ENSAE, & Inria FairPlay",
              "url": ""
            }
          ]
        },
        {
          "author": "Vianney Perchet",
          "authorURL": "",
          "affiliations": [
            {
              "name": "CREST, ENSAE, Inria FairPlay, & Criteo AI Lab",
              "url": ""
            }
          ]
        },
        {
          "author": "Spyros Angelopoulos",
          "authorURL": "",
          "affiliations": [
            {
              "name": "CNRS and ILLS Montreal",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Lorenzo Croissant</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Other</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/projects/">Projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/events/">Events</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/github/">GitHub</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Algorithms with Predictions</h1> <p>How to use your excel spreadsheet to make decisions.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#what-are-the-core-problems">What are the core problems?</a></div> <div><a href="#example-the-one-max-search-problem">Example: the one-max-search problem</a></div> <div><a href="#open-questions-for-learning-theorists">Open questions for learning theorists</a></div> </nav> </d-contents> <div style="display:none"> $$ \def\de{\mathrm{d}} \def\De{\mathrm{D}} \def\x{\times} \def\ve{\varepsilon} \def\dre{\delta r^\ve} \def\de{\mathrm{d}} \def\De{\mathrm{D}} \def\x{\times} \def\ve{\varepsilon} \def\dre{\delta r^\ve} \def\1{\mathbb{1}} \def\argmax{\mathrm{arg}\max} $$ $$ \def\Ab{\mathbb{A}} \def\Bb{\mathbb{B}} \def\Cb{\mathbb{C}} \def\Db{\mathbb{D}} \def\Eb{\mathbb{E}} \def\Fb{\mathbb{F}} \def\Hb{\mathbb{H}} \def\Gb{\mathbb{G}} \def\Ib{\mathbb{I}} \def\Jb{\mathbb{J}} \def\Lb{\mathbb{L}} \def\Kb{\mathbb{K}} \def\Mb{\mathbb{M}} \def\Nb{\mathbb{N}} \def\Ob{\mathbb{O}} \def\Pb{\mathbb{P}} \def\Qb{\mathbb{Q}} \def\Rb{\mathbb{R}} \def\Sb{\mathbb{S}} \def\Tb{\mathbb{T}} \def\Ub{\mathbb{U}} \def\Vb{\mathbb{V}} \def\Wb{\mathbb{W}} \def\Xb{\mathbb{X}} \def\Yb{\mathbb{Y}} \def\Zb{\mathbb{Z}} $$ $$ \def\Ac{\mathcal{A}} \def\Bc{\mathcal{B}} \def\Cc{\mathcal{C}} \def\Dc{\mathcal{D}} \def\Ec{\mathcal{E}} \def\Fc{\mathcal{F}} \def\Hc{\mathcal{H}} \def\Gc{\mathcal{G}} \def\Ic{\mathcal{I}} \def\Jc{\mathcal{J}} \def\Lc{\mathcal{L}} \def\Kc{\mathcal{K}} \def\Mc{\mathcal{M}} \def\Nc{\mathcal{N}} \def\Oc{\mathcal{O}} \def\Pc{\mathcal{P}} \def\Qc{\mathcal{Q}} \def\Rc{\mathcal{R}} \def\Sc{\mathcal{S}} \def\Tc{\mathcal{T}} \def\Uc{\mathcal{U}} \def\Vc{\mathcal{V}} \def\Wc{\mathcal{W}} \def\Xc{\mathcal{X}} \def\Yc{\mathcal{Y}} \def\Zc{\mathcal{Z}} $$ $$ \def\Ar{\mathrm{A}} \def\Br{\mathrm{B}} \def\Cr{\mathrm{C}} \def\Dr{\mathrm{D}} \def\Er{\mathrm{E}} \def\Fr{\mathrm{F}} \def\Hr{\mathrm{H}} \def\Gr{\mathrm{G}} \def\Ir{\mathrm{I}} \def\Jr{\mathrm{J}} \def\Lr{\mathrm{L}} \def\Kr{\mathrm{K}} \def\Mr{\mathrm{M}} \def\Nr{\mathrm{N}} \def\Or{\mathrm{O}} \def\Pr{\mathrm{P}} \def\Qr{\mathrm{Q}} \def\Rr{\mathrm{R}} \def\Sr{\mathrm{S}} \def\Tr{\mathrm{T}} \def\Ur{\mathrm{U}} \def\Vr{\mathrm{V}} \def\Wr{\mathrm{W}} \def\Xr{\mathrm{X}} \def\Yr{\mathrm{Y}} \def\Zr{\mathrm{Z}} $$ $$ \def\ar{\mathrm{a}} \def\br{\mathrm{b}} \def\cr{\mathrm{c}} \def\dr{\mathrm{d}} \def\er{\mathrm{e}} \def\fr{\mathrm{f}} \def\hr{\mathrm{g}} \def\gr{\mathrm{h}} \def\ir{\mathrm{i}} \def\jr{\mathrm{j}} \def\kr{\mathrm{k}} \def\lr{\mathrm{l}} \def\mr{\mathrm{m}} \def\nr{\mathrm{n}} \def\or{\mathrm{o}} \def\pr{\mathrm{p}} \def\qr{\mathrm{q}} \def\rr{\mathrm{r}} \def\sr{\mathrm{s}} \def\tr{\mathrm{t}} \def\ur{\mathrm{u}} \def\vr{\mathrm{v}} \def\wr{\mathrm{w}} \def\xr{\mathrm{x}} \def\yr{\mathrm{y}} \def\zr{\mathrm{z}} $$ $$ \def\As{\mathscr{A}} \def\Bs{\mathscr{B}} \def\Cs{\mathscr{C}} \def\Ds{\mathscr{D}} \def\Es{\mathscr{E}} \def\Fs{\mathscr{F}} \def\Hs{\mathscr{H}} \def\Gs{\mathscr{G}} \def\Is{\mathscr{I}} \def\Js{\mathscr{J}} \def\Ls{\mathscr{L}} \def\Ks{\mathscr{K}} \def\Ms{\mathscr{M}} \def\Ns{\mathscr{N}} \def\Os{\mathscr{O}} \def\Ps{\mathscr{P}} \def\Qs{\mathscr{Q}} \def\Rs{\mathscr{R}} \def\Ss{\mathscr{S}} \def\Ts{\mathscr{T}} \def\Us{\mathscr{U}} \def\Vs{\mathscr{V}} \def\Ws{\mathscr{W}} \def\Xs{\mathscr{X}} \def\Ys{\mathscr{Y}} \def\Zs{\mathscr{Z}} $$ $$ \def\Abf{\mathbf{A}} \def\Bbf{\mathbf{B}} \def\Cbf{\mathbf{C}} \def\Dbf{\mathbf{D}} \def\Ebf{\mathbf{E}} \def\Fbf{\mathbf{F}} \def\Hbf{\mathbf{H}} \def\Gbf{\mathbf{G}} \def\Ibf{\mathbf{I}} \def\Jbf{\mathbf{J}} \def\Lbf{\mathbf{L}} \def\Kbf{\mathbf{K}} \def\Mbf{\mathbf{M}} \def\Nbf{\mathbf{N}} \def\Obf{\mathbf{O}} \def\Pbf{\mathbf{P}} \def\Qbf{\mathbf{Q}} \def\Rbf{\mathbf{R}} \def\Sbf{\mathbf{S}} \def\Tbf{\mathbf{T}} \def\Ubf{\mathbf{U}} \def\Vbf{\mathbf{V}} \def\Wbf{\mathbf{W}} \def\Xbf{\mathbf{X}} \def\Ybf{\mathbf{Y}} \def\Zbf{\mathbf{Z}} $$ $$ \def\abf{\mathbf{a}} \def\bbf{\mathbf{b}} \def\cbf{\mathbf{c}} \def\dbf{\mathbf{d}} \def\ebf{\mathbf{e}} \def\fbf{\mathbf{f}} \def\hbf{\mathbf{g}} \def\gbf{\mathbf{h}} \def\ibf{\mathbf{i}} \def\jbf{\mathbf{j}} \def\kbf{\mathbf{k}} \def\lbf{\mathbf{l}} \def\mbf{\mathbf{m}} \def\nbf{\mathbf{n}} \def\obf{\mathbf{o}} \def\pbf{\mathbf{p}} \def\qbf{\mathbf{q}} \def\rbf{\mathbf{r}} \def\sbf{\mathbf{s}} \def\tbf{\mathbf{t}} \def\ubf{\mathbf{u}} \def\vbf{\mathbf{v}} \def\wbf{\mathbf{w}} \def\xbf{\mathbf{x}} \def\ybf{\mathbf{y}} \def\zbf{\mathbf{z}} $$ $$ \def\Af{\mathfrak{A}} \def\Bf{\mathfrak{B}} \def\Cf{\mathfrak{C}} \def\Df{\mathfrak{D}} \def\Ef{\mathfrak{E}} \def\Ff{\mathfrak{F}} \def\Hf{\mathfrak{H}} \def\Gf{\mathfrak{G}} \def\If{\mathfrak{I}} \def\Jf{\mathfrak{J}} \def\Lf{\mathfrak{L}} \def\Kf{\mathfrak{K}} \def\Mf{\mathfrak{M}} \def\Nf{\mathfrak{N}} \def\Of{\mathfrak{O}} \def\Pf{\mathfrak{P}} \def\Qf{\mathfrak{Q}} \def\Rf{\mathfrak{R}} \def\Sf{\mathfrak{S}} \def\Tf{\mathfrak{T}} \def\Uf{\mathfrak{U}} \def\Vf{\mathfrak{V}} \def\Wf{\mathfrak{W}} \def\Xf{\mathfrak{X}} \def\Yf{\mathfrak{Y}} \def\Zf{\mathfrak{Z}} $$ $$ \def\af{\mathfrak{a}} \def\bf{\mathfrak{b}} \def\cf{\mathfrak{c}} \def\df{\mathfrak{d}} \def\ef{\mathfrak{e}} \def\ff{\mathfrak{f}} \def\hf{\mathfrak{g}} \def\gf{\mathfrak{h}} \def\if{\mathfrak{i}} \def\jf{\mathfrak{j}} \def\kf{\mathfrak{k}} \def\lf{\mathfrak{l}} \def\mf{\mathfrak{m}} \def\nf{\mathfrak{n}} \def\of{\mathfrak{o}} \def\pf{\mathfrak{p}} \def\qf{\mathfrak{q}} \def\rf{\mathfrak{r}} \def\sf{\mathfrak{s}} \def\tf{\mathfrak{t}} \def\uf{\mathfrak{u}} \def\vf{\mathfrak{v}} \def\wf{\mathfrak{w}} \def\xf{\mathfrak{x}} \def\yf{\mathfrak{y}} \def\zf{\mathfrak{z}} $$ </div> <p>Algorithms with predictions (also called learning‚Äëaugmented or prediction‚Äëaugmented algorithms) augment classical algorithmic procedures with side information about the future, which could be predictions produced by ML models, domain experts, or simple heuristics. The basic philosophy is pragmatic: predictions are available in many real systems (forecasted demand, estimated user behavior, model outputs used by human operators), and algorithms that can use these signals profit from improved average performance while still providing safeguards against bad forecasts.</p> <p>While this problem is primarily studied in the computer science algorithms community, it should interest learning theorists and statisticians, especially those interested in the interface between learning and decision-making. Indeed, informing traditional decision-making is the main way statistical learning is applied in practice. Whether our concern (as an accademic community) is the quality of decision making by policymakers or how to correctly train students to <it>use</it> ML models, it is crucial to expand our understanding of this aspect of decision-making under uncertainty.</p> <h2 id="what-are-the-core-problems">What are the core problems?</h2> <p>Of course, aspects of algorithm design for specific problems are not within the scope of statistical learning theory. However, several conceptual and technical questions arise naturally at the intersection of learning and decision-making that should interest learning theorists. These stem from the key properties that learning-augmented algorithms aim to achieve:</p> <ul> <li> Consistency: when predictions are accurate (or calibrated), a good algorithm should (almost) match the performance of an ideal clairvoyant method (fast, high performance, low regret, etc.).</li> <li> Robustness: when predictions are catastrophically wrong (worst-case), the algorithm should not be much worse than the classical worst‚Äëcase algorithms' guarantees.</li> <li> Smoothness: in between these two extremes, an algorithm's performance should degrade smoothly: small prediction error should cause only small performance loss. Smoothness is a necessary condition for actual deployability, as real predictors will never be perfect.</li> </ul> <p>Naturally, each criterion individually is trivial to optimise: always trust the predictor (perfect consistency) or always ignore it (perfect robustness, and smoothness). The challenge is in the multi-objective problem. The opposition between consistency and robustness for example, is quite self-evident: the heart of the question is in characterising, for a given problem, the shape of the Pareto frontier of achievable trade-offs. Most early work focused only on achieving consistency and robustness, with smoothness being a more recent (but already widely accepted) goal.</p> <p>Further reading and a curated overview are available at the Algorithms with Predictions initiative <a href="https://algorithms-with-predictions.github.io/" target="_blank" rel="noopener noreferrer">website</a>.</p> <h2 id="example-the-one-max-search-problem">Example: The one-max-search problem</h2> <p>As an illustration of the concepts of algorithms with predictions, this section sumarises the contents of <d-cite key="Benomar25"></d-cite> on the problem of one-max-search.</p> <p>The one-max-search problem is a simple online decision problem, but despite its simplicity the prediction augmented problem is already very rich in challenges. One-max-search is an optimal stopping problem generally presented as an optimal execution problem: we have a single asset to sell over a time horizon of $T$ periods, and at each period $t\in[T]$ we observe a price $X_t$ which we can either accept or forfeit to move to $t+1$.</p> <p>This problem is very similar to the Prophet and Secretary problems, so to clear up any confusion, let‚Äôs contrast these optimal stopping problems before moving on. A Prophet problem is characterised by the <it>independent</it> between the prices $X_t$. A Secretary problem is characterised by the <it>randomisation of arrival order</it> of the prices $X_t$, which are otherwise adversarially chosen. In contrast, the prices in one-max-search are adversarially chosen, including their order of arrival. Still, these problems are analysed by similar techniques, including the competitive ratio of an algorithm, defined as the worst-case ratio between the algorithm‚Äôs performance and that of a clairvoyant (a.k.a. offline) planner who knows all prices in advance<d-footnote>Note that the expectation may be outside the ratio in some analyses, see e.g. <d-cite key="lee1988expectation"></d-cite>,<d-cite key="ezra2023prophet"></d-cite>, and references therein. This is a design choice.</d-footnote> \(\begin{align} \text{CR} = \inf_{X_1,\ldots,X_T} \frac{\mathbb{E}[\text{ALG}(X_1,\ldots,X_T)]}{\mathbb{E}[\text{OPT}(X_1,\ldots,X_T)]}. \end{align}\)</p> <p>The notations $\text{ALG}(X_1,\ldots,X_T)$ and $\text{OPT}(X_1,\ldots,X_T)$ are left intentionally vague in this literature to emphasise that the competitive ratio is a very general concept that can be applied to many different problems and performance metrics (e.g., the value $X_t$ chosen for prophets and one-max-search, vs. the probability of selecting the maximum value in secretary problems). Due to independence, the optimal competitive ratio for Prophet problems is $1/2$, see <d-cite key="krengel_semiamarts_1977"></d-cite>, while for Secretary problems it is $1/e$, see <d-cite key="dynkin1963optimum"></d-cite>. In one-max-search, one must assume that the prices are bounded in $[1,\theta]$ for some $\theta&gt;1$ to get non-trivial guarantees, and the optimal competitive ratio is $\theta^{-1/2}$, see <d-cite key="el-yaniv_competitive_1998"></d-cite>.</p> <p>What about predictions? How much can they improve this $\theta^{-1/2}$ ratio? The <it>robust</it> algorithm of El-Yaniv, in <d-cite key="el-yaniv_competitive_1998"></d-cite>, buys the first price above $\sqrt{\theta}$, and gives us a bound on the robustness achievable. At the other end, given a prediction $Y$ of the maximum price $\max_{t\in[T]} X_t$, a consistent algorithm sets its threhold price at $Y$, achieving a consistency of $1$. In between, Sun et al. already gave, in <d-cite key="sun_pareto-optimal_2021"></d-cite>, the Pareto frontier of achievable trade-offs between consistency and robustness. I will spare you the equations for the front, which aren‚Äôt very enlightening.</p> <p>However, the algorithm of Sun et al. is not smooth: a small error in the prediction $Y$ can cause a large drop in performance. In order to obtain smoothness, my coauthors and I (chiefly <a href="https://scholar.google.com/citations?user=ZhWGR7QAAAAJ&amp;hl=fr" target="_blank" rel="noopener noreferrer">Ziyad Benomar</a>) reanalysed the Pareto front to characterise <it>all</it> smooth threshold algorithms through the geometry of the set of Pareto-optimal threshold functions. This formalism allowed us to then study the smoothness of these functions to derive competitive ratio bounds in terms of the prediction error \(\begin{align} \Ec(Y,X^*):= \min\left\{\frac Y{X^*}, \frac{X^*}{Y}\right\}, \end{align}\)</p> <p>where \(X^*:=\max_{t\in[T]} X_t\) is the maximum price. Precisely, we show that the competitive ratio degrades as a power of the prediction error $\Ec(Y,X^*)$ which relates directly to the smoothness of the algorithm. Finding the optimal smoothness can then be characterised through a triple consistency-robustness-smoothness Pareto front.</p> <p>I will leave technical details to the paper, the instructive conclusions in my view are that:</p> <ul> <li>answering the multi-objective problem of consistency, robustness, and smoothness is quite difficult even on simple problems,</li> <li>that, instead of searching for a single optimal algorithm, a keener understanding of the problem's constraints and geometry is needed to conclusively answer the trade-offs,</li> <li>and that, while the high-level ideas transfer well, the nitty-gritty of analysis is ad-hoc and problem dependent and it's not clear if a general form akin to dynamic programming can be found.</li> </ul> <h2 id="open-questions-for-learning-theorists">Open questions for learning theorists</h2> <p>This leaves us with an awkward feeling: the problem of algorithms with predictions is cool, challenging, and highly relevant for practical applications, but its ungeneralisability leaves an unpleasant aftertaste. Perhaps a new perspective is needed, and learning theorists and statisticians are well equiped to provide it. In my view, there are several conceptual and technical questions that are particularly natural for learning theorists and statisticians.</p> <ol type="1"> <li> <b>Bridging prediction error and generalisation guarantees.</b> In algorithms with predictions, analysis is generally worst-case and purely deterministic: the predictor is just some scalar. In contrast, learning theory has cut its teeth on stochastic generalisation frameworks. Can we bring these together to obtain meaningful performance guarantees?</li> <li> <b>From static predictions to control and dynamic programming.</b> Many decision problems are inherently sequential and can be solved via the general dynamic programming principle. The generality of this framework is a huge achievement and a powerful tool. Can we find an analogue general decision framework for algorithms with predictions? In the context of control, this has been studied a bit<d-footnote>See, e.g., papers on <it>lookahead</it> in control and reinforcement learning such as <d-cite key="nadav1"></d-cite> and <d-cite key="nadav2"></d-cite>.</d-footnote>, but a general theory is still missing.</li> <li> <b>Classifying brittleness and stability of problems.</b> There's an interesting question going around in the community: which problems are inherently brittle, i.e., for which <it>no</it> algorithm can be both robust and consistent, while being smooth? See for example <d-cite key="elenter2024overcoming"></d-cite> for a recent example. On the one hand, classifying problems into brittle and stable families is an interesting theoretical question, but perhaps one ill suited to statistics. But, on the other hand, us learning theorists should probably be considering the consequences of this brittleness when predicting information for decision-making problems. </li> <li> <b>Distributional predictions.</b> On many problems, point predictions are not the most natural output of a predictor. One would be better served by a prediction about the whole distribution of a key variable, such as $X^*$ in one-max-search. There is a strong demand for the study of this problem from computer scientists, who often find their tools inadequate for the task. There's strong potential for statisticians and applied mathematicials to contribute here. One might hasard to guess conections to questions of calibration in learning theory</li> </ol> <p>This area is still new and emerging, but already very active. If you are interested in any of the above questions, feel free to reach out, I can put you in contact with other researchers in the area. If you want to learn more about algorithms with predictions, I highly recomend the curated Algorithms with Predictions initiative <a href="https://algorithms-with-predictions.github.io/" target="_blank" rel="noopener noreferrer">website</a>, every research field should have one!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Lorenzo Croissant. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos by myself. Last updated: October 17, 2025. </div> </footer> <d-bibliography src="/assets/bibliography/ALPS.bib"></d-bibliography> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>